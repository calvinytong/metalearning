{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metalearning Notes\n",
    "*Calvin Tong, George Austin 2018-09-17*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Metalearning is the idea that one can learn to learn by solving many tasks. As humans, this is something we do all the time and one of the halmarks of intelligence. For example, when we see one example of a handwritten symbol, we can immediatly recognize most other examples. We do this without thinking, but this property does not carry over to modern machine learning models. There is still a lot of work to do to get to this ideal of learning how to learn, but in a sense if we solve this problem, then we will essentially solve all supervised machine learning problems. The first attempts at solving this problem reframe it in the supervised learning case, which means that each training case is treated as a training task.\n",
    "\n",
    "Some success stories: \n",
    "\n",
    "* OMNI dataset given a new character recognize it's class: we've acheived 98% accuracy on this dataset (Mishra et al. 2017)\n",
    "* Neural architecture search: Find the right architecture (Zoph and Le 2017)\n",
    "* Hindsight Experience Replay (almost meta learning): Solve a hard problem by making it harder. This is from the RL world, so its easiest framed in movement contexts. Let's say the goal is to reach a certain state A. You create a model that tries to reach A, but reaches B instead. When this happens you can use this to learn now to reach state B. \n",
    "* Sim2Real: Allow for transfer from simulation to real robots. This often fails because there are different factors not accounted for in the simulation. The idea is to vary the parameters of the simulation and train a policy that can adapt to these situations \n",
    "\n",
    "The idea is to learn parameters $\\phi$ such that the model will have low loss given $k$ training iterations. More formally, given a task $\\tau$ with loss function $L_\\tau$ we preform the minimization\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\underset{\\phi}{\\text{minimize}} && \\mathbb{E} [L_\\tau (U_\\tau^k(\\phi))]\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "where $U_\\tau^k$ is an operator defining k updates to $\\phi$ using data sampled from $\\tau$.\n",
    "\n",
    "\n",
    "For this presentation, we're going to focus on two recent, similar algorithms, [Model-Agnostic Meta-Learning](https://arxiv.org/pdf/1703.03400.pdf) or MAML and it's extension [Reptile](https://arxiv.org/pdf/1803.02999.pdf). In both of these algorithms there is a metalearner and a learner. The learner is trained by the metalearner on tasks drawn from a task distribution $P(\\tau)$, and then fine tuned with a few task specific examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General ML Background Notes\n",
    "* Interesting view of Backprop: \n",
    "Find the best circuit given constraints where each data point represents a constraint $F(x_i; \\theta) = y_i$\n",
    "\n",
    "* Reinforcement learning\n",
    "    * Find a policy that maximises reward, i.e. we maximize $\\mathbb{E}[\\underset{t}{\\sum} r_t]$\n",
    "    * At a high level: add some randomess to your actions, if your result was better than expected do more in the future repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Convergence](images/convergence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a somewhat informal argument from the paper on why this method works. The argument is that the algorithms converge to a vector of parameters that is close in Euclidean distance to each task's manifold of optimal solutions. As such we define the problem as \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{\\phi}{\\text{minimize}} && \\mathbb{E_\\tau} [\\frac{1}{2} D (\\phi, W_{\\tau})^2]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $W_{\\tau}$ is the set of optimal paramaters for a task $\\tau$ and D is the euclidean distance function. We introduce the $\\frac{1}{2}$ to make the math easier later. \n",
    "\n",
    "In order to deal with this, we have to review some math. Let's define a non-pathological set $S \\subset R^d$ and $\\phi \\in \\mathbb{R}^d$. Given that we are working with an appropriatly well behaved subset of $\\mathbb{R}^d$, the gradient of the squared distance function $D(\\phi, S)^2$ can be well approximated by $2(\\phi - proj_S(\\phi))$ (TODO: Flush this fact out more intuitivly, something having to do with distance being the min over all points in the set of absolute distances). Recall that this projection is just the closest value (in the euclidean sense) to the vector in the set. Once we have this approximation, it becomes clear that we can rewrite the gradient of our objective function as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\phi \\mathbb{E_\\tau} [\\frac{1}{2} D (\\phi, W_{\\tau})] &= \\mathbb{E_\\tau} [\\frac{1}{2} \\nabla_\\phi D (\\phi, W_{\\tau})] \\\\\n",
    "&= \\mathbb{E_\\tau} [\\phi - proj_{W_{\\tau}}(\\phi)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So we can rewrite our gradient update as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi &\\leftarrow \\phi - \\alpha \\nabla_\\phi \\frac{1}{2} D (\\phi, W_{\\tau})^2 \\\\\n",
    "&\\leftarrow \\phi - \\alpha (\\phi - proj_{W_{\\tau}}(\\phi)) \\\\\n",
    "&\\leftarrow (1-\\alpha) \\phi - \\alpha proj_{W_{\\tau}}(\\phi)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Even though we can't compute $proj_{W_{\\tau}}(\\phi)$ because it requires us to find the set of minimizers for the given task, we can approximate it with gradient decent. So, for each interation of reptile we see that we sample a task and replace $W_{\\tau}$ with the result of running k steps of gradient decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reptile in Action\n",
    "Reptile is the successor to MAML and acheives similar preformance, but is easier to implement and faster at runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain of the sin function\n",
    "x_all = np.linspace(-5, 5, 50)\n",
    "\n",
    "# Number of training iterations\n",
    "n_iter = 30000\n",
    "\n",
    "# Number of inner training iterations\n",
    "n_iter_inner = 1\n",
    "\n",
    "# Number of training mini batches\n",
    "n_train = 10\n",
    "\n",
    "# Alpha for inner and outer SGD\n",
    "alpha_inner = 0.02\n",
    "alpha_outer = 0.1\n",
    "\n",
    "# Ploting parameters\n",
    "eval_epoch = 8\n",
    "num_plots = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Build simple feedforward network in keras\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(64, activation='tanh', input_shape=(1,)))\n",
    "    model.add(keras.layers.Dense(64, activation='tanh'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    sgd = keras.optimizers.SGD(lr=alpha_inner)\n",
    "    model.compile(optimizer=sgd,\n",
    "                  loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def gen_task():\n",
    "    \"\"\"\n",
    "    Create regression problem\n",
    "    \"\"\"\n",
    "    phase = np.random.uniform(0, 2*np.pi)\n",
    "    ampl = np.random.uniform(0.1, 5)\n",
    "    f_randomsine = lambda x : np.sin(x + phase) * ampl\n",
    "    return f_randomsine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = build_model()\n",
    "\n",
    "# Evaluation task\n",
    "f_eval = gen_task()\n",
    "\n",
    "for iteration in range(n_iter + 1):\n",
    "    # Save original weights\n",
    "    weights_old = np.array(model.get_weights())\n",
    "    \n",
    "    # Generate task\n",
    "    f = gen_task()\n",
    "    y_all = f(x_all)\n",
    "    \n",
    "    # Train on task with SGD\n",
    "    inds = np.random.permutation(len(x_all))\n",
    "    for s in range(0, len(x_all), n_train):   \n",
    "        model.fit(x_all[s:s+n_train], y_all[s:s+n_train], epochs=n_iter_inner, verbose=0)\n",
    "        \n",
    "    # Update weights with pseudogradient\n",
    "    weights_new = np.array(model.get_weights())\n",
    "    outerstepsize = alpha_outer * (1 - iteration / n_iter)\n",
    "    update = weights_old + outerstepsize * (weights_new - weights_old)\n",
    "    model.set_weights(update.tolist())\n",
    "    \n",
    "    # Evaluate every 2500 epochs\n",
    "    if iteration % 5000 == 0:\n",
    "        # Save weights before\n",
    "        weights_before = model.get_weights()\n",
    "        \n",
    "        # Plot initial predictions\n",
    "        plt.figure()\n",
    "        plt.plot(x_all, f_eval(x_all), label=\"truth\")\n",
    "        plt.plot(x_all, model.predict(x_all), label=\"pred after 0\")\n",
    "        \n",
    "        # Train for eval_epoch and plot\n",
    "        for i in range(num_plots):\n",
    "            model.fit(x_all, f_eval(x_all), epochs=eval_epoch // num_plots, verbose=0)\n",
    "            plt.plot(x_all, model.predict(x_all), label=\"pred after \" + str(((i+1) * eval_epoch)))\n",
    "        plt.legend()\n",
    "        plt.title(\"training after \" + str(iteration))     \n",
    "        \n",
    "        # Restore weights from before\n",
    "        model.set_weights(weights_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "Good talk by Ilya Sutskeverv https://www.youtube.com/watch?v=AopSlxNYqX8&frags=pl%2Cwn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
